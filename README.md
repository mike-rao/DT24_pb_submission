Push Battle is a challenge to develop an intelligent agent that is able to master the art of positioning, adapting to shifting board dynamics, and anticipating the opponent’s moves.

Our Push Battle AI agent competes in the game Push Battle, an evolved Tic-Tac-Toe game where the board is 8x8 and two-players aim to create rows of three pieces by placing and moving their pieces. The twist is that pieces can be pushed around when other pieces are placed within close proximity and pieces can overflow to the opposite side of the board. The agent uses a layered strategy to analyze the game board, anticipate the opponent’s moves, and choose its actions for the best possible outcome. Starting with a strong placement strategy and progressing to tactical movement, the AI adapts to dynamic board conditions and pursues winning configurations with a calculated mix of offense and defense.

Our AI leverages key components of artificial intelligence, such as the minimax algorithm with alpha-beta pruning, to navigate and evaluate possible moves within the game’s time constraints (5 seconds per move). Initially, we developed the agent with a robust minimax-based approach to simulate possible moves and determine optimal strategies. The alpha-beta pruning further enhances this by reducing computation time, allowing the agent to explore deeper strategies within the limited timeframe.

Building on the minimax foundation, we implemented a specialized agent, an offensive strategy that attempts to secure a quick victory by occupying critical board positions in the first three moves. This agent tries to establish an early winning configuration by aiming for two opposite corners, creating a quick path to victory if uncontested. If this initial agent encounters obstacles, it defers to the general minimax strategy to adapt.

Major challenges include balancing offense and defense which required tuning and testing to ensure the agent wasn’t overly conservative or reckless. Handling the game’s "push" mechanic, where piece placements shift the position of adjacent pieces, added further complexity, requiring careful tracking of potential board states. Incorporating the initial agent strategy presented its own hurdles; this offensive approach worked best under specific conditions, so we had to develop a fallback that ensures smooth transitions to our minimax-based tactics when obstacles arise. Testing and refining the agent’s heuristic board evaluation algorithm can improve the agent's defensive capabilities which seem to lack the most.
